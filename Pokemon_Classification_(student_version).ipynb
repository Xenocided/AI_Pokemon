{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pokemon Classification (student_version).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxIVAyQufgVw"
      },
      "source": [
        "# Import the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9emAP3KLfnhS"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import * \n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "from matplotlib import image as img\n",
        "\n",
        "LOCAL = not 'CLOUDSDK_CONFIG' in os.environ\n",
        "\n",
        "if not LOCAL:\n",
        "  \n",
        "  from google.colab import files, drive, output\n",
        "\n",
        "\n",
        " # drive.mount('/content/gdrive')\n",
        " # os.chdir('/content/gdrive/My Drive/workshop/pokemon classification/demo')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp6d-yjVgH7D"
      },
      "source": [
        "## Download the Pokemon dataset\n",
        "We will first download the archive of the dataset and then unzip it. The image dataset are directly re-used from PyImageSearch https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns. The blog post has some great discussion on the Convolutional Neural Network (CNN). Do take a look when you have time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yk_6hTnpgHq"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nZisCOxpI22lXU2iBbv-x-uFVEzBKIll' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1nZisCOxpI22lXU2iBbv-x-uFVEzBKIll\" -O dataset.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q dataset.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYjeoHTX5AN4"
      },
      "source": [
        "ls dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r3RUmLkjo_q"
      },
      "source": [
        "## Building Model\n",
        "Here we will build our Convolutional Neural Network (CNN) using Keras APIs and TensorFlow backend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIP5TbqOM0ho"
      },
      "source": [
        "###Plot some images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3JeG4iRM4Ai"
      },
      "source": [
        "sample = list()\n",
        "size = 3\n",
        "i = 0\n",
        "figure = plt.figure(figsize=(20,8))\n",
        "\n",
        "for item in os.listdir('dataset/'):\n",
        "  if item!='README':\n",
        "    imagedir = os.path.join('dataset/',item)\n",
        "    imageitem = os.listdir(imagedir)\n",
        "    subset = np.random.choice(imageitem, 3, replace=False)\n",
        "    \n",
        "    for image_sample in subset:\n",
        "      ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "      ax.set_title(imagedir.split('/')[1], fontsize=20)\n",
        "      ax.imshow(img.imread( os.path.join(imagedir,image_sample),0 ))\n",
        "      i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epl26PBpgJZe"
      },
      "source": [
        "### Generate training/evalaution dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIny4bagNvX"
      },
      "source": [
        "IMG_SIZE_HEIGHT = 96\n",
        "IMG_SIZE_WIDTH = 96\n",
        "\n",
        "data_generator = image.ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
        "\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\"./dataset\",\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(\n",
        "                                                     IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH),\n",
        "                                                     color_mode='rgb',\n",
        "                                                     class_mode='categorical',\n",
        "                                                     subset=\"training\",\n",
        "                                                     seed=1\n",
        "                                                     )\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\"./dataset\",\n",
        "                                                          shuffle=True,\n",
        "                                                          target_size=(\n",
        "                                                          IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH),\n",
        "                                                          color_mode='rgb',\n",
        "                                                          class_mode='categorical',\n",
        "                                                          subset=\"validation\",\n",
        "                                                          seed=1\n",
        "                                                          )\n",
        "                                                  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHobpz1Zf41r"
      },
      "source": [
        "### Defines  the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcjt6noh-MvK"
      },
      "source": [
        "clear_session()\n",
        "\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "# building the CNN model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gjfi-F3NBxi"
      },
      "source": [
        "We used the softmax function to map the network's output scores into class probabilities:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwT0z0ItMogf"
      },
      "source": [
        "$$ softmax(s_i) = \\frac{\\exp(s_i)}{\\sum_{j=1}^K \\exp(s_j)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rlVGvWSgQ3g"
      },
      "source": [
        "### Fitting the data into the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98qAWiZK3l1h"
      },
      "source": [
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=1,restore_best_weights=True)\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 300\n",
        "\n",
        "\n",
        "# Fitting the data into the model\n",
        "result = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[es],\n",
        ")\n",
        "\n",
        "\n",
        "# Wait for training completion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4e12Tz1v_oe"
      },
      "source": [
        "pd.DataFrame(result.history)[['loss','val_loss']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LILo322Xwu4l"
      },
      "source": [
        "pd.DataFrame(result.history)[['accuracy','val_accuracy']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYYZrACnajhF"
      },
      "source": [
        "filename = 'model.hdf5'\n",
        "\n",
        "\n",
        "model.save(filename)\n",
        "#model = load_model(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ZqHykNMDG9"
      },
      "source": [
        "res = model.evaluate(validation_generator)\n",
        "names = model.metrics_names\n",
        "for i in range(len(res)):\n",
        "  print(f'{names[i]}: {res[i]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y7C9y4ytKZB"
      },
      "source": [
        "## Making Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ZiSNV9tJ1b"
      },
      "source": [
        "if not LOCAL:\n",
        "  # Create folder for file upload\n",
        "  if not os.path.exists('./data/unknown'):\n",
        "      os.makedirs('./data/unknown')\n",
        "  else:\n",
        "      shutil.rmtree('./data/unknown')\n",
        "      os.makedirs('./data/unknown')\n",
        "\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for file_name in uploaded:\n",
        "  # file_name = list(uploaded)[0]\n",
        "  # print(file_name)\n",
        "    shutil.move(file_name, os.path.join('./data/unknown', file_name))\n",
        "\n",
        "    directory_path =  \"./data\"\n",
        "else:\n",
        "    directory_path =  \"./test_data\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl2RWBlxt0rb"
      },
      "source": [
        "predict_datagen = image.ImageDataGenerator(rescale=1./255)\n",
        "predict_generator =  predict_datagen.flow_from_directory(\n",
        "        directory_path,\n",
        "        target_size=(IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH),\n",
        "        color_mode=\"rgb\",\n",
        "        class_mode='categorical',\n",
        "        batch_size=1)\n",
        "\n",
        "filenames = predict_generator.filenames\n",
        "predictions = model.predict(predict_generator)\n",
        "\n",
        "\n",
        "\n",
        "prediction = np.argmax(predictions, axis=1)\n",
        "\n",
        "label_map = (train_generator.class_indices)\n",
        "label_map = dict((v,k) for k,v in label_map.items())\n",
        "prediction = [label_map[k] for k in prediction]\n",
        "\n",
        "filenames_predictions_map = list(zip(filenames, prediction))\n",
        "print (filenames_predictions_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v51p_bah29R"
      },
      "source": [
        "figure = plt.figure(figsize=(25,10))\n",
        "for i, (filename, pred_class) in enumerate(filenames_predictions_map):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "\n",
        "   \n",
        "    ax.set_title(\"{} ({:.2f}%) ({})\".format(\n",
        "                                  prediction[i], \n",
        "                                  np.max(predictions[i])*100,\n",
        "                                  filename),\n",
        "                 \n",
        "                )\n",
        "    ax.imshow(img.imread(os.path.join('./data/',filename)))\n",
        "                                  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_lAxi0KHgkY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}